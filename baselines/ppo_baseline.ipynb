{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\XPS\\Desktop\\github_projects\\cs224r_final_project\\2048\\Lib\\site-packages\\gymnasium_2048\\envs\\twenty_forty_eight.py:250: RuntimeWarning: overflow encountered in scalar add\n",
      "  self.total_score += self.step_score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Avg Reward: 7.16\n",
      "Iter 1, Avg Reward: 7.64\n",
      "Iter 2, Avg Reward: 7.55\n",
      "Iter 3, Avg Reward: 7.62\n",
      "Iter 4, Avg Reward: 7.98\n",
      "Iter 5, Avg Reward: 7.25\n",
      "Iter 6, Avg Reward: 7.88\n",
      "Iter 7, Avg Reward: 8.13\n",
      "Iter 8, Avg Reward: 7.45\n",
      "Iter 9, Avg Reward: 7.48\n",
      "Iter 10, Avg Reward: 7.62\n",
      "Iter 11, Avg Reward: 8.09\n",
      "Iter 12, Avg Reward: 7.45\n",
      "Iter 13, Avg Reward: 7.89\n",
      "Iter 14, Avg Reward: 7.37\n",
      "Iter 15, Avg Reward: 7.79\n",
      "Iter 16, Avg Reward: 7.52\n",
      "Iter 17, Avg Reward: 8.11\n",
      "Iter 18, Avg Reward: 7.70\n",
      "Iter 19, Avg Reward: 7.90\n",
      "Iter 20, Avg Reward: 7.58\n",
      "Iter 21, Avg Reward: 7.14\n",
      "Iter 22, Avg Reward: 7.80\n",
      "Iter 23, Avg Reward: 7.84\n",
      "Iter 24, Avg Reward: 7.63\n",
      "Iter 25, Avg Reward: 7.54\n",
      "Iter 26, Avg Reward: 7.28\n",
      "Iter 27, Avg Reward: 6.71\n",
      "Iter 28, Avg Reward: 7.11\n",
      "Iter 29, Avg Reward: 7.28\n",
      "Iter 30, Avg Reward: 7.50\n",
      "Iter 31, Avg Reward: 7.64\n",
      "Iter 32, Avg Reward: 7.34\n",
      "Iter 33, Avg Reward: 6.91\n",
      "Iter 34, Avg Reward: 7.32\n",
      "Iter 35, Avg Reward: 7.07\n",
      "Iter 36, Avg Reward: 7.83\n",
      "Iter 37, Avg Reward: 7.45\n",
      "Iter 38, Avg Reward: 7.92\n",
      "Iter 39, Avg Reward: 7.84\n",
      "Iter 40, Avg Reward: 7.83\n",
      "Iter 41, Avg Reward: 7.69\n",
      "Iter 42, Avg Reward: 7.49\n",
      "Iter 43, Avg Reward: 7.62\n",
      "Iter 44, Avg Reward: 7.61\n",
      "Iter 45, Avg Reward: 7.76\n",
      "Iter 46, Avg Reward: 7.32\n",
      "Iter 47, Avg Reward: 7.42\n",
      "Iter 48, Avg Reward: 7.67\n",
      "Iter 49, Avg Reward: 7.47\n",
      "Iter 50, Avg Reward: 7.83\n",
      "Iter 51, Avg Reward: 7.37\n",
      "Iter 52, Avg Reward: 7.07\n",
      "Iter 53, Avg Reward: 7.94\n",
      "Iter 54, Avg Reward: 7.66\n",
      "Iter 55, Avg Reward: 7.67\n",
      "Iter 56, Avg Reward: 7.01\n",
      "Iter 57, Avg Reward: 7.06\n",
      "Iter 58, Avg Reward: 7.42\n",
      "Iter 59, Avg Reward: 7.51\n",
      "Iter 60, Avg Reward: 7.93\n",
      "Iter 61, Avg Reward: 7.86\n",
      "Iter 62, Avg Reward: 7.53\n",
      "Iter 63, Avg Reward: 7.01\n",
      "Iter 64, Avg Reward: 7.60\n",
      "Iter 65, Avg Reward: 7.17\n",
      "Iter 66, Avg Reward: 7.57\n",
      "Iter 67, Avg Reward: 7.66\n",
      "Iter 68, Avg Reward: 7.97\n",
      "Iter 69, Avg Reward: 6.76\n",
      "Iter 70, Avg Reward: 7.75\n",
      "Iter 71, Avg Reward: 7.37\n",
      "Iter 72, Avg Reward: 7.94\n",
      "Iter 73, Avg Reward: 7.43\n",
      "Iter 74, Avg Reward: 7.21\n",
      "Iter 75, Avg Reward: 7.68\n",
      "Iter 76, Avg Reward: 7.23\n",
      "Iter 77, Avg Reward: 7.51\n",
      "Iter 78, Avg Reward: 7.59\n",
      "Iter 79, Avg Reward: 7.02\n",
      "Iter 80, Avg Reward: 7.52\n",
      "Iter 81, Avg Reward: 7.31\n",
      "Iter 82, Avg Reward: 7.25\n",
      "Iter 83, Avg Reward: 6.93\n",
      "Iter 84, Avg Reward: 7.25\n",
      "Iter 85, Avg Reward: 6.92\n",
      "Iter 86, Avg Reward: 6.56\n",
      "Iter 87, Avg Reward: 6.96\n",
      "Iter 88, Avg Reward: 6.68\n",
      "Iter 89, Avg Reward: 4.94\n",
      "Iter 90, Avg Reward: 6.64\n",
      "Iter 91, Avg Reward: 6.92\n",
      "Iter 92, Avg Reward: 6.84\n",
      "Iter 93, Avg Reward: 6.13\n",
      "Iter 94, Avg Reward: 6.24\n",
      "Iter 95, Avg Reward: 6.08\n",
      "Iter 96, Avg Reward: 5.68\n",
      "Iter 97, Avg Reward: 6.88\n",
      "Iter 98, Avg Reward: 6.09\n",
      "Iter 99, Avg Reward: 6.78\n",
      "Iter 100, Avg Reward: 6.41\n",
      "Iter 101, Avg Reward: 6.67\n",
      "Iter 102, Avg Reward: 6.73\n",
      "Iter 103, Avg Reward: 7.21\n",
      "Iter 104, Avg Reward: 7.16\n",
      "Iter 105, Avg Reward: 7.08\n",
      "Iter 106, Avg Reward: 7.67\n",
      "Iter 107, Avg Reward: 7.77\n",
      "Iter 108, Avg Reward: 7.59\n",
      "Iter 109, Avg Reward: 7.81\n",
      "Iter 110, Avg Reward: 7.45\n",
      "Iter 111, Avg Reward: 7.56\n",
      "Iter 112, Avg Reward: 7.85\n",
      "Iter 113, Avg Reward: 7.41\n",
      "Iter 114, Avg Reward: 8.20\n",
      "Iter 115, Avg Reward: 7.33\n",
      "Iter 116, Avg Reward: 7.26\n",
      "Iter 117, Avg Reward: 8.11\n",
      "Iter 118, Avg Reward: 7.36\n",
      "Iter 119, Avg Reward: 7.54\n",
      "Iter 120, Avg Reward: 6.71\n",
      "Iter 121, Avg Reward: 7.02\n",
      "Iter 122, Avg Reward: 6.96\n",
      "Iter 123, Avg Reward: 6.86\n",
      "Iter 124, Avg Reward: 7.63\n",
      "Iter 125, Avg Reward: 6.88\n",
      "Iter 126, Avg Reward: 7.23\n",
      "Iter 127, Avg Reward: 7.24\n",
      "Iter 128, Avg Reward: 6.69\n",
      "Iter 129, Avg Reward: 7.25\n",
      "Iter 130, Avg Reward: 6.74\n",
      "Iter 131, Avg Reward: 7.37\n",
      "Iter 132, Avg Reward: 7.20\n",
      "Iter 133, Avg Reward: 7.12\n",
      "Iter 134, Avg Reward: 7.81\n",
      "Iter 135, Avg Reward: 8.10\n",
      "Iter 136, Avg Reward: 6.94\n",
      "Iter 137, Avg Reward: 7.42\n",
      "Iter 138, Avg Reward: 7.55\n",
      "Iter 139, Avg Reward: 7.12\n",
      "Iter 140, Avg Reward: 6.96\n",
      "Iter 141, Avg Reward: 7.76\n",
      "Iter 142, Avg Reward: 7.44\n",
      "Iter 143, Avg Reward: 7.56\n",
      "Iter 144, Avg Reward: 7.89\n",
      "Iter 145, Avg Reward: 7.58\n",
      "Iter 146, Avg Reward: 7.68\n",
      "Iter 147, Avg Reward: 7.60\n",
      "Iter 148, Avg Reward: 7.25\n",
      "Iter 149, Avg Reward: 7.47\n",
      "Iter 150, Avg Reward: 7.36\n",
      "Iter 151, Avg Reward: 7.43\n",
      "Iter 152, Avg Reward: 6.98\n",
      "Iter 153, Avg Reward: 7.67\n",
      "Iter 154, Avg Reward: 6.96\n",
      "Iter 155, Avg Reward: 7.13\n",
      "Iter 156, Avg Reward: 7.47\n",
      "Iter 157, Avg Reward: 7.55\n",
      "Iter 158, Avg Reward: 7.41\n",
      "Iter 159, Avg Reward: 7.29\n",
      "Iter 160, Avg Reward: 7.16\n",
      "Iter 161, Avg Reward: 6.85\n",
      "Iter 162, Avg Reward: 7.02\n",
      "Iter 163, Avg Reward: 7.30\n",
      "Iter 164, Avg Reward: 6.95\n",
      "Iter 165, Avg Reward: 6.71\n",
      "Iter 166, Avg Reward: 5.36\n",
      "Iter 167, Avg Reward: 6.98\n",
      "Iter 168, Avg Reward: 7.22\n",
      "Iter 169, Avg Reward: 6.95\n",
      "Iter 170, Avg Reward: 7.65\n",
      "Iter 171, Avg Reward: 7.28\n",
      "Iter 172, Avg Reward: 6.85\n",
      "Iter 173, Avg Reward: 6.14\n",
      "Iter 174, Avg Reward: 6.57\n",
      "Iter 175, Avg Reward: 6.74\n",
      "Iter 176, Avg Reward: 6.75\n",
      "Iter 177, Avg Reward: 6.72\n",
      "Iter 178, Avg Reward: 6.49\n",
      "Iter 179, Avg Reward: 5.53\n",
      "Iter 180, Avg Reward: 4.66\n",
      "Iter 181, Avg Reward: 6.72\n",
      "Iter 182, Avg Reward: 6.93\n",
      "Iter 183, Avg Reward: 6.83\n",
      "Iter 184, Avg Reward: 6.98\n",
      "Iter 185, Avg Reward: 6.91\n",
      "Iter 186, Avg Reward: 7.17\n",
      "Iter 187, Avg Reward: 5.68\n",
      "Iter 188, Avg Reward: 3.71\n",
      "Iter 189, Avg Reward: 6.22\n",
      "Iter 190, Avg Reward: 6.05\n",
      "Iter 191, Avg Reward: 6.11\n",
      "Iter 192, Avg Reward: 5.41\n",
      "Iter 193, Avg Reward: 5.12\n",
      "Iter 194, Avg Reward: 5.02\n",
      "Iter 195, Avg Reward: 3.30\n",
      "Iter 196, Avg Reward: 5.02\n",
      "Iter 197, Avg Reward: 6.52\n",
      "Iter 198, Avg Reward: 7.04\n",
      "Iter 199, Avg Reward: 6.01\n",
      "\n",
      "=== Final Move Frequency (Total) ===\n",
      "Up: 93577\n",
      "Right: 81084\n",
      "Down: 89540\n",
      "Left: 145399\n",
      "\n",
      "=== Move Distribution Per 10% of Training ===\n",
      "\n",
      "--- Iterations 1 to 20 ---\n",
      "Up: 10137\n",
      "Right: 10010\n",
      "Down: 9759\n",
      "Left: 11054\n",
      "\n",
      "--- Iterations 21 to 40 ---\n",
      "Up: 9833\n",
      "Right: 10518\n",
      "Down: 10494\n",
      "Left: 10115\n",
      "\n",
      "--- Iterations 41 to 60 ---\n",
      "Up: 10686\n",
      "Right: 10663\n",
      "Down: 9519\n",
      "Left: 10092\n",
      "\n",
      "--- Iterations 61 to 80 ---\n",
      "Up: 8435\n",
      "Right: 9381\n",
      "Down: 11429\n",
      "Left: 11715\n",
      "\n",
      "--- Iterations 81 to 100 ---\n",
      "Up: 6223\n",
      "Right: 5419\n",
      "Down: 6619\n",
      "Left: 22699\n",
      "\n",
      "--- Iterations 101 to 120 ---\n",
      "Up: 11777\n",
      "Right: 8666\n",
      "Down: 7817\n",
      "Left: 12700\n",
      "\n",
      "--- Iterations 121 to 140 ---\n",
      "Up: 7628\n",
      "Right: 16068\n",
      "Down: 8045\n",
      "Left: 9219\n",
      "\n",
      "--- Iterations 141 to 160 ---\n",
      "Up: 10355\n",
      "Right: 9670\n",
      "Down: 8778\n",
      "Left: 12157\n",
      "\n",
      "--- Iterations 161 to 180 ---\n",
      "Up: 9008\n",
      "Right: 482\n",
      "Down: 9634\n",
      "Left: 21836\n",
      "\n",
      "--- Iterations 181 to 200 ---\n",
      "Up: 9495\n",
      "Right: 207\n",
      "Down: 7446\n",
      "Left: 23812\n",
      "\n",
      "üîç Evaluating agent over 100 episodes...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 158\u001b[39m\n\u001b[32m    156\u001b[39m state = preprocess(obs)\n\u001b[32m    157\u001b[39m logits, _ = model(state)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m dist = \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m action = dist.probs.argmax().item()\n\u001b[32m    161\u001b[39m move_name = ACTION_MAP[action]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\XPS\\Desktop\\github_projects\\cs224r_final_project\\2048\\Lib\\site-packages\\torch\\distributions\\categorical.py:69\u001b[39m, in \u001b[36mCategorical.__init__\u001b[39m\u001b[34m(self, probs, logits, validate_args)\u001b[39m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mself\u001b[39m.logits = logits - logits.logsumexp(dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     68\u001b[39m \u001b[38;5;28mself\u001b[39m._param = \u001b[38;5;28mself\u001b[39m.probs \u001b[38;5;28;01mif\u001b[39;00m probs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.logits\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28mself\u001b[39m._num_events = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_param\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[-\u001b[32m1\u001b[39m]\n\u001b[32m     70\u001b[39m batch_shape = (\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mself\u001b[39m._param.size()[:-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._param.ndimension() > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch.Size()\n\u001b[32m     72\u001b[39m )\n\u001b[32m     73\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(batch_shape, validate_args=validate_args)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_2048\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"gymnasium_2048/TwentyFortyEight-v0\")\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.95\n",
    "CLIP_EPS = 0.2\n",
    "LR = 3e-4\n",
    "PPO_EPOCHS = 4\n",
    "ROLLOUT_STEPS = 2048\n",
    "TOTAL_ITERATIONS = 200\n",
    "\n",
    "ACTION_MAP = {0: \"Up\", 1: \"Right\", 2: \"Down\", 3: \"Left\"}\n",
    "global_move_counter = defaultdict(int)\n",
    "bucket_move_counters = []\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def preprocess(state):\n",
    "    flat = np.reshape(state, -1)\n",
    "    return torch.tensor(flat, dtype=torch.float32).to(device)\n",
    "\n",
    "# --- Actor-Critic Network ---\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy = nn.Linear(64, 4)\n",
    "        self.value = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        return self.policy(x), self.value(x)\n",
    "\n",
    "model = ActorCritic().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# --- Collect Rollout ---\n",
    "def collect_rollout():\n",
    "    states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "    local_move_counter = defaultdict(int)\n",
    "    obs, _ = env.reset()\n",
    "    for _ in range(ROLLOUT_STEPS):\n",
    "        state = preprocess(obs)\n",
    "        logits, value = model(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "\n",
    "        move_name = ACTION_MAP[action.item()]\n",
    "        global_move_counter[move_name] += 1\n",
    "        local_move_counter[move_name] += 1\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(torch.tensor(reward, dtype=torch.float32, device=device))\n",
    "        dones.append(torch.tensor(done, dtype=torch.float32, device=device))\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value.squeeze())\n",
    "\n",
    "        obs = next_obs if not done else env.reset()[0]\n",
    "    return states, actions, rewards, dones, log_probs, values, local_move_counter\n",
    "\n",
    "# --- Compute GAE and Returns ---\n",
    "def compute_advantages(rewards, values, dones):\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    values = values + [torch.tensor(0.0).to(device)]\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + GAMMA * values[t+1] * (1 - dones[t]) - values[t]\n",
    "        gae = delta + GAMMA * LAMBDA * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "    returns = [adv + val for adv, val in zip(advantages, values[:-1])]\n",
    "    return advantages, returns\n",
    "\n",
    "# --- Training Loop ---\n",
    "for iteration in range(TOTAL_ITERATIONS):\n",
    "    states, actions, rewards, dones, old_log_probs, values, local_move_counter = collect_rollout()\n",
    "    advantages, returns = compute_advantages(rewards, values, dones)\n",
    "\n",
    "    states = torch.stack(states)\n",
    "    actions = torch.stack(actions)\n",
    "    old_log_probs = torch.stack(old_log_probs).detach()\n",
    "    advantages = torch.stack(advantages).detach()\n",
    "    returns = torch.stack(returns).detach()\n",
    "\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    for _ in range(PPO_EPOCHS):\n",
    "        logits, value = model(states)\n",
    "        dist = Categorical(logits=logits)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        ratio = (new_log_probs - old_log_probs).exp()\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = (returns - value.squeeze()).pow(2).mean()\n",
    "\n",
    "        loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_reward = torch.sum(torch.stack(rewards)) / len(rewards)\n",
    "    print(f\"Iter {iteration}, Avg Reward: {avg_reward.item():.2f}\")\n",
    "\n",
    "    if (iteration + 1) % (TOTAL_ITERATIONS // 10) == 0:\n",
    "        bucket_move_counters.append(dict(global_move_counter))\n",
    "\n",
    "# --- Final Move Summary ---\n",
    "print(\"\\n=== Final Move Frequency (Total) ===\")\n",
    "for move in ACTION_MAP.values():\n",
    "    print(f\"{move}: {global_move_counter[move]}\")\n",
    "\n",
    "print(\"\\n=== Move Distribution Per 10% of Training ===\")\n",
    "for i, snapshot in enumerate(bucket_move_counters):\n",
    "    print(f\"\\n--- Iterations {i * (TOTAL_ITERATIONS // 10) + 1} to {(i + 1) * (TOTAL_ITERATIONS // 10)} ---\")\n",
    "    if i == 0:\n",
    "        diff = snapshot\n",
    "    else:\n",
    "        prev = bucket_move_counters[i - 1]\n",
    "        diff = {move: snapshot[move] - prev.get(move, 0) for move in ACTION_MAP.values()}\n",
    "    for move in ACTION_MAP.values():\n",
    "        print(f\"{move}: {diff[move]}\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"\\nüîç Evaluating agent over 100 episodes...\")\n",
    "eval_scores = []\n",
    "eval_move_counter = defaultdict(int)\n",
    "tile_counts = Counter()\n",
    "\n",
    "for _ in range(100):\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        state = preprocess(obs)\n",
    "        logits, _ = model(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.probs.argmax().item()\n",
    "\n",
    "        move_name = ACTION_MAP[action]\n",
    "        eval_move_counter[move_name] += 1\n",
    "\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "\n",
    "    eval_scores.append(total_reward)\n",
    "    final_board = np.argmax(obs, axis=2)\n",
    "    tile_counts[2 ** np.max(final_board)] += 1\n",
    "\n",
    "print(\"\\n‚úÖ Evaluation Results:\")\n",
    "print(f\"Average Score: {np.mean(eval_scores):.2f}\")\n",
    "print(f\"Max Score: {np.max(eval_scores):.2f}\")\n",
    "print(\"\\nMax Tile Frequencies:\")\n",
    "for tile, count in sorted(tile_counts.items(), reverse=True):\n",
    "    print(f\"{tile}: {count} times\")\n",
    "\n",
    "print(\"\\nEvaluation Move Distribution:\")\n",
    "for move in ACTION_MAP.values():\n",
    "    print(f\"{move}: {eval_move_counter[move]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2048",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
