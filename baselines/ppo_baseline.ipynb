{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/10000, Last Score: 1084.00\n",
      "Iter 2/10000, Last Score: 1168.00\n",
      "Iter 3/10000, Last Score: 464.00\n",
      "Iter 4/10000, Last Score: 1436.00\n",
      "Iter 5/10000, Last Score: 640.00\n",
      "Iter 6/10000, Last Score: 180.00\n",
      "Iter 7/10000, Last Score: 300.00\n",
      "Iter 8/10000, Last Score: 596.00\n",
      "Iter 9/10000, Last Score: 992.00\n",
      "Iter 10/10000, Last Score: 1276.00\n",
      "Iter 11/10000, Last Score: 840.00\n",
      "Iter 12/10000, Last Score: 1344.00\n",
      "Iter 13/10000, Last Score: 580.00\n",
      "Iter 14/10000, Last Score: 920.00\n",
      "Iter 15/10000, Last Score: 1016.00\n",
      "Iter 16/10000, Last Score: 1468.00\n",
      "Iter 17/10000, Last Score: 1340.00\n",
      "Iter 18/10000, Last Score: 780.00\n",
      "Iter 19/10000, Last Score: 1140.00\n",
      "Iter 20/10000, Last Score: 1356.00\n",
      "Iter 21/10000, Last Score: 824.00\n",
      "Iter 22/10000, Last Score: 1360.00\n",
      "Iter 23/10000, Last Score: 744.00\n",
      "Iter 24/10000, Last Score: 584.00\n",
      "Iter 25/10000, Last Score: 1308.00\n",
      "Iter 26/10000, Last Score: 612.00\n",
      "Iter 27/10000, Last Score: 1440.00\n",
      "Iter 28/10000, Last Score: 568.00\n",
      "Iter 29/10000, Last Score: 636.00\n",
      "Iter 30/10000, Last Score: 1004.00\n",
      "Iter 31/10000, Last Score: 1088.00\n",
      "Iter 32/10000, Last Score: 940.00\n",
      "Iter 33/10000, Last Score: 512.00\n",
      "Iter 34/10000, Last Score: 1224.00\n",
      "Iter 35/10000, Last Score: 756.00\n",
      "Iter 36/10000, Last Score: 628.00\n",
      "Iter 37/10000, Last Score: 1400.00\n",
      "Iter 38/10000, Last Score: 512.00\n",
      "Iter 39/10000, Last Score: 1356.00\n",
      "Iter 40/10000, Last Score: 1188.00\n",
      "Iter 41/10000, Last Score: 864.00\n",
      "Iter 42/10000, Last Score: 760.00\n",
      "Iter 43/10000, Last Score: 1300.00\n",
      "Iter 44/10000, Last Score: 1348.00\n",
      "Iter 45/10000, Last Score: 740.00\n",
      "Iter 46/10000, Last Score: 792.00\n",
      "Iter 47/10000, Last Score: 2364.00\n",
      "Iter 48/10000, Last Score: 1424.00\n",
      "Iter 49/10000, Last Score: 352.00\n",
      "Iter 50/10000, Last Score: 1304.00\n",
      "Iter 51/10000, Last Score: 1080.00\n",
      "Iter 52/10000, Last Score: 1280.00\n",
      "Iter 53/10000, Last Score: 1416.00\n",
      "Iter 54/10000, Last Score: 1460.00\n",
      "Iter 55/10000, Last Score: 368.00\n",
      "Iter 56/10000, Last Score: 716.00\n",
      "Iter 57/10000, Last Score: 1412.00\n",
      "Iter 58/10000, Last Score: 1320.00\n",
      "Iter 59/10000, Last Score: 1428.00\n",
      "Iter 60/10000, Last Score: 2056.00\n",
      "Iter 61/10000, Last Score: 308.00\n",
      "Iter 62/10000, Last Score: 484.00\n",
      "Iter 63/10000, Last Score: 564.00\n",
      "Iter 64/10000, Last Score: 624.00\n",
      "Iter 65/10000, Last Score: 1200.00\n",
      "Iter 66/10000, Last Score: 1068.00\n",
      "Iter 67/10000, Last Score: 1468.00\n",
      "Iter 68/10000, Last Score: 1328.00\n",
      "Iter 69/10000, Last Score: 892.00\n",
      "Iter 70/10000, Last Score: 2092.00\n",
      "Iter 71/10000, Last Score: 388.00\n",
      "Iter 72/10000, Last Score: 1124.00\n",
      "Iter 73/10000, Last Score: 836.00\n",
      "Iter 74/10000, Last Score: 1016.00\n",
      "Iter 75/10000, Last Score: 384.00\n",
      "Iter 76/10000, Last Score: 648.00\n",
      "Iter 77/10000, Last Score: 1376.00\n",
      "Iter 78/10000, Last Score: 860.00\n",
      "Iter 79/10000, Last Score: 1188.00\n",
      "Iter 80/10000, Last Score: 644.00\n",
      "Iter 81/10000, Last Score: 1132.00\n",
      "Iter 82/10000, Last Score: 568.00\n",
      "Iter 83/10000, Last Score: 1400.00\n",
      "Iter 84/10000, Last Score: 2124.00\n",
      "Iter 85/10000, Last Score: 544.00\n",
      "Iter 86/10000, Last Score: 1224.00\n",
      "Iter 87/10000, Last Score: 616.00\n",
      "Iter 88/10000, Last Score: 864.00\n",
      "Iter 89/10000, Last Score: 1148.00\n",
      "Iter 90/10000, Last Score: 1768.00\n",
      "Iter 91/10000, Last Score: 888.00\n",
      "Iter 92/10000, Last Score: 304.00\n",
      "Iter 93/10000, Last Score: 664.00\n",
      "Iter 94/10000, Last Score: 2172.00\n",
      "Iter 95/10000, Last Score: 1668.00\n",
      "Iter 96/10000, Last Score: 1384.00\n",
      "Iter 97/10000, Last Score: 1052.00\n",
      "Iter 98/10000, Last Score: 1488.00\n",
      "Iter 99/10000, Last Score: 1472.00\n",
      "Iter 100/10000, Last Score: 1080.00\n",
      "Iter 101/10000, Last Score: 1648.00\n",
      "Iter 102/10000, Last Score: 860.00\n",
      "Iter 103/10000, Last Score: 916.00\n",
      "Iter 104/10000, Last Score: 1220.00\n",
      "Iter 105/10000, Last Score: 952.00\n",
      "Iter 106/10000, Last Score: 1280.00\n",
      "Iter 107/10000, Last Score: 1264.00\n",
      "Iter 108/10000, Last Score: 1076.00\n",
      "Iter 109/10000, Last Score: 1700.00\n",
      "Iter 110/10000, Last Score: 696.00\n",
      "Iter 111/10000, Last Score: 1212.00\n",
      "Iter 112/10000, Last Score: 924.00\n",
      "Iter 113/10000, Last Score: 672.00\n",
      "Iter 114/10000, Last Score: 1572.00\n",
      "Iter 115/10000, Last Score: 2128.00\n",
      "Iter 116/10000, Last Score: 292.00\n",
      "Iter 117/10000, Last Score: 448.00\n",
      "Iter 118/10000, Last Score: 640.00\n",
      "Iter 119/10000, Last Score: 620.00\n",
      "Iter 120/10000, Last Score: 1388.00\n",
      "Iter 121/10000, Last Score: 1280.00\n",
      "Iter 122/10000, Last Score: 808.00\n",
      "Iter 123/10000, Last Score: 712.00\n",
      "Iter 124/10000, Last Score: 604.00\n",
      "Iter 125/10000, Last Score: 876.00\n",
      "Iter 126/10000, Last Score: 1204.00\n",
      "Iter 127/10000, Last Score: 1096.00\n",
      "Iter 128/10000, Last Score: 1572.00\n",
      "Iter 129/10000, Last Score: 644.00\n",
      "Iter 130/10000, Last Score: 1396.00\n",
      "Iter 131/10000, Last Score: 1136.00\n",
      "Iter 132/10000, Last Score: 704.00\n",
      "Iter 133/10000, Last Score: 656.00\n",
      "Iter 134/10000, Last Score: 872.00\n",
      "Iter 135/10000, Last Score: 1492.00\n",
      "Iter 136/10000, Last Score: 1320.00\n",
      "Iter 137/10000, Last Score: 1704.00\n",
      "Iter 138/10000, Last Score: 596.00\n",
      "Iter 139/10000, Last Score: 636.00\n",
      "Iter 140/10000, Last Score: 2212.00\n",
      "Iter 141/10000, Last Score: 1308.00\n",
      "Iter 142/10000, Last Score: 1448.00\n",
      "Iter 143/10000, Last Score: 1320.00\n",
      "Iter 144/10000, Last Score: 820.00\n",
      "Iter 145/10000, Last Score: 1172.00\n",
      "Iter 146/10000, Last Score: 1388.00\n",
      "Iter 147/10000, Last Score: 520.00\n",
      "Iter 148/10000, Last Score: 732.00\n",
      "Iter 149/10000, Last Score: 1332.00\n",
      "Iter 150/10000, Last Score: 1348.00\n",
      "Iter 151/10000, Last Score: 820.00\n",
      "Iter 152/10000, Last Score: 964.00\n",
      "Iter 153/10000, Last Score: 1032.00\n",
      "Iter 154/10000, Last Score: 1440.00\n",
      "Iter 155/10000, Last Score: 576.00\n",
      "Iter 156/10000, Last Score: 1344.00\n",
      "Iter 157/10000, Last Score: 1372.00\n",
      "Iter 158/10000, Last Score: 1808.00\n",
      "Iter 159/10000, Last Score: 944.00\n",
      "Iter 160/10000, Last Score: 1464.00\n",
      "Iter 161/10000, Last Score: 1312.00\n",
      "Iter 162/10000, Last Score: 1948.00\n",
      "Iter 163/10000, Last Score: 2396.00\n",
      "Iter 164/10000, Last Score: 1372.00\n",
      "Iter 165/10000, Last Score: 364.00\n",
      "Iter 166/10000, Last Score: 2576.00\n",
      "Iter 167/10000, Last Score: 1144.00\n",
      "Iter 168/10000, Last Score: 684.00\n",
      "Iter 169/10000, Last Score: 464.00\n",
      "Iter 170/10000, Last Score: 1132.00\n",
      "Iter 171/10000, Last Score: 672.00\n",
      "Iter 172/10000, Last Score: 1440.00\n",
      "Iter 173/10000, Last Score: 880.00\n",
      "Iter 174/10000, Last Score: 2108.00\n",
      "Iter 175/10000, Last Score: 1220.00\n",
      "Iter 176/10000, Last Score: 1012.00\n",
      "Iter 177/10000, Last Score: 2232.00\n",
      "Iter 178/10000, Last Score: 1036.00\n",
      "Iter 179/10000, Last Score: 632.00\n",
      "Iter 180/10000, Last Score: 1444.00\n",
      "Iter 181/10000, Last Score: 1008.00\n",
      "Iter 182/10000, Last Score: 496.00\n",
      "Iter 183/10000, Last Score: 1436.00\n",
      "Iter 184/10000, Last Score: 2108.00\n",
      "Iter 185/10000, Last Score: 792.00\n",
      "Iter 186/10000, Last Score: 672.00\n",
      "Iter 187/10000, Last Score: 568.00\n",
      "Iter 188/10000, Last Score: 2048.00\n",
      "Iter 189/10000, Last Score: 1368.00\n",
      "Iter 190/10000, Last Score: 2072.00\n",
      "Iter 191/10000, Last Score: 2276.00\n",
      "Iter 192/10000, Last Score: 1704.00\n",
      "Iter 193/10000, Last Score: 1720.00\n",
      "Iter 194/10000, Last Score: 3640.00\n",
      "Iter 195/10000, Last Score: 1496.00\n",
      "Iter 196/10000, Last Score: 2660.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\XPS\\Desktop\\github_projects\\cs224r_final_project\\2048\\Lib\\site-packages\\gymnasium_2048\\envs\\twenty_forty_eight.py:158: RuntimeWarning: overflow encountered in scalar add\n",
      "  score += 2 ** (board[row, col] + 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 197/10000, Last Score: 1480.00\n",
      "Iter 198/10000, Last Score: 1356.00\n",
      "Iter 199/10000, Last Score: 1084.00\n",
      "Iter 200/10000, Last Score: 1668.00\n",
      "Iter 201/10000, Last Score: 732.00\n",
      "Iter 202/10000, Last Score: 1432.00\n",
      "Iter 203/10000, Last Score: 1516.00\n",
      "Iter 204/10000, Last Score: 1748.00\n",
      "Iter 205/10000, Last Score: 1960.00\n",
      "Iter 206/10000, Last Score: 2808.00\n",
      "Iter 207/10000, Last Score: 1028.00\n",
      "Iter 208/10000, Last Score: 2960.00\n",
      "Iter 209/10000, Last Score: 2196.00\n",
      "Iter 210/10000, Last Score: 3024.00\n",
      "Iter 211/10000, Last Score: 416.00\n",
      "Iter 212/10000, Last Score: 1240.00\n",
      "Iter 213/10000, Last Score: 2200.00\n",
      "Iter 214/10000, Last Score: 4276.00\n",
      "Iter 215/10000, Last Score: 1156.00\n",
      "Iter 216/10000, Last Score: 872.00\n",
      "Iter 217/10000, Last Score: 1652.00\n",
      "Iter 218/10000, Last Score: 1252.00\n",
      "Iter 219/10000, Last Score: 2452.00\n",
      "Iter 220/10000, Last Score: 1068.00\n",
      "Iter 221/10000, Last Score: 408.00\n",
      "Iter 222/10000, Last Score: 1332.00\n",
      "Iter 223/10000, Last Score: 1328.00\n",
      "Iter 224/10000, Last Score: 1124.00\n",
      "Iter 225/10000, Last Score: 996.00\n",
      "Iter 226/10000, Last Score: 1020.00\n",
      "Iter 227/10000, Last Score: 2384.00\n",
      "Iter 228/10000, Last Score: 2828.00\n",
      "Iter 229/10000, Last Score: 2500.00\n",
      "Iter 230/10000, Last Score: 2108.00\n",
      "Iter 231/10000, Last Score: 856.00\n",
      "Iter 232/10000, Last Score: 2676.00\n",
      "Iter 233/10000, Last Score: 560.00\n",
      "Iter 234/10000, Last Score: 1440.00\n",
      "Iter 235/10000, Last Score: 1892.00\n",
      "Iter 236/10000, Last Score: 2960.00\n",
      "Iter 237/10000, Last Score: 2848.00\n",
      "Iter 238/10000, Last Score: 2224.00\n",
      "Iter 239/10000, Last Score: 712.00\n",
      "Iter 240/10000, Last Score: 2364.00\n",
      "Iter 241/10000, Last Score: 1956.00\n",
      "Iter 242/10000, Last Score: 2836.00\n",
      "Iter 243/10000, Last Score: 1252.00\n",
      "Iter 244/10000, Last Score: 944.00\n",
      "Iter 245/10000, Last Score: 4384.00\n",
      "Iter 246/10000, Last Score: 3176.00\n",
      "Iter 247/10000, Last Score: 1604.00\n",
      "Iter 248/10000, Last Score: 3380.00\n",
      "Iter 249/10000, Last Score: 3660.00\n",
      "Iter 250/10000, Last Score: 3152.00\n",
      "Iter 251/10000, Last Score: 3060.00\n",
      "Iter 252/10000, Last Score: 1276.00\n",
      "Iter 253/10000, Last Score: 1120.00\n",
      "Iter 254/10000, Last Score: 3036.00\n",
      "Iter 255/10000, Last Score: 3284.00\n",
      "Iter 256/10000, Last Score: 3204.00\n",
      "Iter 257/10000, Last Score: 1260.00\n",
      "Iter 258/10000, Last Score: 3508.00\n",
      "Iter 259/10000, Last Score: 1596.00\n",
      "Iter 260/10000, Last Score: 2200.00\n",
      "Iter 261/10000, Last Score: 2256.00\n",
      "Iter 262/10000, Last Score: 3072.00\n",
      "Iter 263/10000, Last Score: 1400.00\n",
      "Iter 264/10000, Last Score: 2000.00\n",
      "Iter 265/10000, Last Score: 1792.00\n",
      "Iter 266/10000, Last Score: 1656.00\n",
      "Iter 267/10000, Last Score: 2716.00\n",
      "Iter 268/10000, Last Score: 3200.00\n",
      "Iter 269/10000, Last Score: 1140.00\n",
      "Iter 270/10000, Last Score: 2156.00\n",
      "Iter 271/10000, Last Score: 3252.00\n",
      "Iter 272/10000, Last Score: 1720.00\n",
      "Iter 273/10000, Last Score: 2224.00\n",
      "Iter 274/10000, Last Score: 1836.00\n",
      "Iter 275/10000, Last Score: 2024.00\n",
      "Iter 276/10000, Last Score: 2156.00\n",
      "Iter 277/10000, Last Score: 1896.00\n",
      "Iter 278/10000, Last Score: 1148.00\n",
      "Iter 279/10000, Last Score: 4552.00\n",
      "Iter 280/10000, Last Score: 1720.00\n",
      "Iter 281/10000, Last Score: 1968.00\n",
      "Iter 282/10000, Last Score: 1464.00\n",
      "Iter 283/10000, Last Score: 3116.00\n",
      "Iter 284/10000, Last Score: 1748.00\n",
      "Iter 285/10000, Last Score: 1320.00\n",
      "Iter 286/10000, Last Score: 1308.00\n",
      "Stopping early at iter 286: rolling avg = 2004.16\n",
      "\n",
      "=== Final Move Frequency (Total) ===\n",
      "Up: 251827\n",
      "Right: 170282\n",
      "Down: 61507\n",
      "Left: 102112\n",
      "\n",
      "=== Move Distribution Per 10% of Training ===\n",
      "\n",
      "🔍 Evaluating over 100 episodes...\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_2048\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import defaultdict, Counter, deque\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"gymnasium_2048/TwentyFortyEight-v0\")\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "GAMMA = 0.99                   # Discount factor\n",
    "LAMBDA = 0.95                  # GAE lambda\n",
    "CLIP_EPS = 0.2                 # PPO clip epsilon\n",
    "LR = 3e-4                      # Learning rate\n",
    "PPO_EPOCHS = 4                 # PPO epochs per update\n",
    "ROLLOUT_STEPS = 2048           # Steps per rollout\n",
    "MAX_ITERATIONS = 10000         # Max training iterations\n",
    "ROLLING_AVG_WINDOW = 100       # Window size for rolling average\n",
    "ROLLING_AVG_THRESHOLD = 2000   # Threshold to stop early\n",
    "DEBUG_TRAIN_ITERS = 0          # Number of debug iterations\n",
    "\n",
    "ACTION_MAP = {0: \"Up\", 1: \"Right\", 2: \"Down\", 3: \"Left\"}\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def preprocess(state):\n",
    "    flat = np.reshape(state, -1)\n",
    "    return torch.tensor(flat, dtype=torch.float32).to(device)\n",
    "\n",
    "# --- Board Decoding ---\n",
    "def decode_board(obs):\n",
    "    \"\"\"Convert observation to 4x4 board of tile values.\"\"\"\n",
    "    if obs.ndim == 3:\n",
    "        idxs = np.argmax(obs, axis=-1)\n",
    "        mask = (obs.sum(axis=-1) == 1)\n",
    "        board = (2 ** idxs) * mask\n",
    "    else:\n",
    "        board = obs\n",
    "    return board\n",
    "\n",
    "# --- Actor-Critic Network ---\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy = nn.Linear(64, 4)\n",
    "        self.value = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        return self.policy(x), self.value(x)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = ActorCritic().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# --- Trackers ---\n",
    "global_move_counter = defaultdict(int)\n",
    "bucket_move_counters = []\n",
    "score_history = deque(maxlen=ROLLING_AVG_WINDOW)\n",
    "\n",
    "# --- Collect Rollout ---\n",
    "def collect_rollout(debug=False):\n",
    "    states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "    local_move_counter = defaultdict(int)\n",
    "    episode_scores = []\n",
    "    episode_debug_info = []\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    current_score = 0.0\n",
    "    current_episode_steps = []\n",
    "\n",
    "    for _ in range(ROLLOUT_STEPS):\n",
    "        if debug:\n",
    "            current_episode_steps.append({\n",
    "                'state': obs.copy(),\n",
    "                'score_before': current_score\n",
    "            })\n",
    "\n",
    "        # act\n",
    "        state = preprocess(obs)\n",
    "        logits, value = model(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        # step\n",
    "        next_obs, step_reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        current_score += step_reward\n",
    "\n",
    "        move_name = ACTION_MAP[action.item()]\n",
    "        local_move_counter[move_name] += 1\n",
    "\n",
    "        # store\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        # **use step reward here**:\n",
    "        rewards.append(torch.tensor(step_reward, dtype=torch.float32))\n",
    "        dones.append(torch.tensor(done, dtype=torch.float32))\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value.squeeze())\n",
    "\n",
    "        if debug:\n",
    "            info = current_episode_steps[-1]\n",
    "            info.update({\n",
    "                'action': move_name,\n",
    "                'step_reward': step_reward,\n",
    "                'state_after': next_obs.copy(),\n",
    "                'score_after': current_score\n",
    "            })\n",
    "\n",
    "        # episode termination\n",
    "        if done:\n",
    "            episode_scores.append(current_score)\n",
    "            if debug and not episode_debug_info:\n",
    "                episode_debug_info = current_episode_steps.copy()\n",
    "            obs, _ = env.reset()\n",
    "            current_score = 0.0\n",
    "            current_episode_steps = []\n",
    "        else:\n",
    "            obs = next_obs\n",
    "\n",
    "    # if no episode finished in this rollout, still record the partial score\n",
    "    if not episode_scores:\n",
    "        episode_scores.append(current_score)\n",
    "\n",
    "    return states, actions, rewards, dones, log_probs, values, local_move_counter, episode_scores, episode_debug_info\n",
    "\n",
    "\n",
    "# --- Compute GAE and Returns ---\n",
    "def compute_advantages(rewards, values, dones):\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    values = values + [torch.tensor(0.0).to(device)]\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + GAMMA * values[t+1] * (1 - dones[t]) - values[t]\n",
    "        gae = delta + GAMMA * LAMBDA * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "    returns = [adv + val for adv, val in zip(advantages, values[:-1])]\n",
    "    return advantages, returns\n",
    "\n",
    "# --- Training Loop ---\n",
    "for iteration in range(MAX_ITERATIONS):\n",
    "    debug = iteration < DEBUG_TRAIN_ITERS\n",
    "    data = collect_rollout(debug)\n",
    "    states, actions, rewards, dones, old_log_probs, values, local_move_counter, episode_scores, debug_info = data\n",
    "\n",
    "    # Update move counters\n",
    "    for move, cnt in local_move_counter.items():\n",
    "        global_move_counter[move] += cnt\n",
    "    if (iteration + 1) % (MAX_ITERATIONS // 10) == 0:\n",
    "        bucket_move_counters.append(local_move_counter.copy())\n",
    "\n",
    "    # Compute advantages and returns\n",
    "    advantages, returns = compute_advantages(rewards, values, dones)\n",
    "    states = torch.stack(states)\n",
    "    actions = torch.stack(actions)\n",
    "    old_log_probs = torch.stack(old_log_probs).detach()\n",
    "    advantages = torch.stack(advantages).detach()\n",
    "    returns = torch.stack(returns).detach()\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    # PPO updates\n",
    "    for _ in range(PPO_EPOCHS):\n",
    "        logits, value = model(states)\n",
    "        dist = Categorical(logits=logits)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "        ratio = (new_log_probs - old_log_probs).exp()\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = (returns - value.squeeze()).pow(2).mean()\n",
    "        loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    last_score = episode_scores[-1] if episode_scores else 0.0\n",
    "    score_history.append(last_score)\n",
    "    print(f\"Iter {iteration+1}/{MAX_ITERATIONS}, Last Score: {last_score:.2f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if len(score_history) == ROLLING_AVG_WINDOW:\n",
    "        rolling_avg = sum(score_history) / ROLLING_AVG_WINDOW\n",
    "        if rolling_avg >= ROLLING_AVG_THRESHOLD:\n",
    "            print(f\"Stopping early at iter {iteration+1}: rolling avg = {rolling_avg:.2f}\")\n",
    "            break\n",
    "\n",
    "# --- Final Move Summary ---\n",
    "print(\"\\n=== Final Move Frequency (Total) ===\")\n",
    "for move in ACTION_MAP.values():\n",
    "    print(f\"{move}: {global_move_counter[move]}\")\n",
    "\n",
    "print(\"\\n=== Move Distribution Per 10% of Training ===\")\n",
    "for i, snap in enumerate(bucket_move_counters):\n",
    "    start = i * (MAX_ITERATIONS // 10) + 1\n",
    "    end = (i + 1) * (MAX_ITERATIONS // 10)\n",
    "    print(f\"\\n--- Iterations {start} to {end} ---\")\n",
    "    prev = bucket_move_counters[i-1] if i>0 else None\n",
    "    diff = {move: snap.get(move,0) - (prev.get(move,0) if prev else 0) for move in ACTION_MAP.values()}\n",
    "    for move, cnt in diff.items():\n",
    "        print(f\"{move}: {cnt}\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"\\n🔍 Evaluating over 100 episodes...\")\n",
    "eval_scores, eval_move_counter, tile_counts = [], defaultdict(int), Counter()\n",
    "for ep in range(100):\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        state = preprocess(obs)\n",
    "        logits, _ = model(state)\n",
    "        action = Categorical(logits=logits).probs.argmax().item()\n",
    "        move = ACTION_MAP[action]\n",
    "        eval_move_counter[move] += 1\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "    eval_scores.append(total_reward)\n",
    "    max_tile = decode_board(obs).max()\n",
    "    tile_counts[max_tile] += 1\n",
    "\n",
    "print(\"\\n✅ Eval Results:\")\n",
    "print(f\"Avg Score: {np.mean(eval_scores):.2f}, Max Score: {np.max(eval_scores):.2f}\")\n",
    "print(\"\\nMax Tile Frequencies:\")\n",
    "for tile, cnt in sorted(tile_counts.items(), reverse=True):\n",
    "    print(f\"{tile}: {cnt} times\")\n",
    "print(\"\\nEval Move Distribution:\")\n",
    "for move in ACTION_MAP.values():\n",
    "    print(f\"{move}: {eval_move_counter[move]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import gymnasium_2048\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import defaultdict, Counter, deque\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = gym.make(\"gymnasium_2048/TwentyFortyEight-v0\")\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "GAMMA = 0.99                   # Discount factor\n",
    "LAMBDA = 0.95                  # GAE lambda\n",
    "CLIP_EPS = 0.2                 # PPO clip epsilon\n",
    "LR = 3e-4                      # Learning rate\n",
    "PPO_EPOCHS = 4                 # PPO epochs per update\n",
    "ROLLOUT_STEPS = 2048           # Steps per rollout\n",
    "MAX_ITERATIONS = 10000         # Max training iterations\n",
    "ROLLING_AVG_WINDOW = 100       # Window size for rolling average\n",
    "ROLLING_AVG_THRESHOLD = 10000   # Threshold to stop early\n",
    "DEBUG_TRAIN_ITERS = 0          # Number of debug iterations\n",
    "EVAL_ITERS = 50\n",
    "\n",
    "ACTION_MAP = {0: \"Up\", 1: \"Right\", 2: \"Down\", 3: \"Left\"}\n",
    "\n",
    "# --- Preprocessing ---\n",
    "def preprocess(state):\n",
    "    flat = np.reshape(state, -1)\n",
    "    return torch.tensor(flat, dtype=torch.float32).to(device)\n",
    "\n",
    "# --- Board Decoding ---\n",
    "def decode_board(obs):\n",
    "    \"\"\"Convert observation to 4x4 board of tile values.\"\"\"\n",
    "    if obs.ndim == 3:\n",
    "        idxs = np.argmax(obs, axis=-1)\n",
    "        mask = (obs.sum(axis=-1) == 1)\n",
    "        board = (2 ** idxs) * mask\n",
    "    else:\n",
    "        board = obs\n",
    "    return board\n",
    "\n",
    "# --- Actor-Critic Network ---\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.policy = nn.Linear(64, 4)\n",
    "        self.value = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        return self.policy(x), self.value(x)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = ActorCritic().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# --- Trackers ---\n",
    "global_move_counter = defaultdict(int)\n",
    "bucket_move_counters = []\n",
    "score_history = deque(maxlen=ROLLING_AVG_WINDOW)\n",
    "\n",
    "# --- Collect Rollout ---\n",
    "def collect_rollout(debug=False):\n",
    "    states, actions, rewards, dones, log_probs, values = [], [], [], [], [], []\n",
    "    local_move_counter = defaultdict(int)\n",
    "    episode_scores = []\n",
    "    episode_debug_info = []\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    current_score = 0.0\n",
    "    current_episode_steps = []\n",
    "\n",
    "    for _ in range(ROLLOUT_STEPS):\n",
    "        if debug:\n",
    "            current_episode_steps.append({\n",
    "                'state': obs.copy(),\n",
    "                'score_before': current_score\n",
    "            })\n",
    "\n",
    "        # act\n",
    "        state = preprocess(obs)\n",
    "        logits, value = model(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        # step\n",
    "        next_obs, step_reward, terminated, truncated, _ = env.step(action.item())\n",
    "        done = terminated or truncated\n",
    "        current_score += step_reward\n",
    "\n",
    "        move_name = ACTION_MAP[action.item()]\n",
    "        local_move_counter[move_name] += 1\n",
    "\n",
    "        # store\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        # **use step reward here**:\n",
    "        rewards.append(torch.tensor(step_reward, dtype=torch.float32))\n",
    "        dones.append(torch.tensor(done, dtype=torch.float32))\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value.squeeze())\n",
    "\n",
    "        if debug:\n",
    "            info = current_episode_steps[-1]\n",
    "            info.update({\n",
    "                'action': move_name,\n",
    "                'step_reward': step_reward,\n",
    "                'state_after': next_obs.copy(),\n",
    "                'score_after': current_score\n",
    "            })\n",
    "\n",
    "        # episode termination\n",
    "        if done:\n",
    "            episode_scores.append(current_score)\n",
    "            if debug and not episode_debug_info:\n",
    "                episode_debug_info = current_episode_steps.copy()\n",
    "            obs, _ = env.reset()\n",
    "            current_score = 0.0\n",
    "            current_episode_steps = []\n",
    "        else:\n",
    "            obs = next_obs\n",
    "\n",
    "    # if no episode finished in this rollout, still record the partial score\n",
    "    if not episode_scores:\n",
    "        episode_scores.append(current_score)\n",
    "\n",
    "    return states, actions, rewards, dones, log_probs, values, local_move_counter, episode_scores, episode_debug_info\n",
    "\n",
    "\n",
    "# --- Compute GAE and Returns ---\n",
    "def compute_advantages(rewards, values, dones):\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    values = values + [torch.tensor(0.0).to(device)]\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + GAMMA * values[t+1] * (1 - dones[t]) - values[t]\n",
    "        gae = delta + GAMMA * LAMBDA * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "    returns = [adv + val for adv, val in zip(advantages, values[:-1])]\n",
    "    return advantages, returns\n",
    "\n",
    "# --- Training Loop ---\n",
    "for iteration in range(MAX_ITERATIONS):\n",
    "    debug = iteration < DEBUG_TRAIN_ITERS\n",
    "    data = collect_rollout(debug)\n",
    "    states, actions, rewards, dones, old_log_probs, values, local_move_counter, episode_scores, debug_info = data\n",
    "\n",
    "    # Update move counters\n",
    "    for move, cnt in local_move_counter.items():\n",
    "        global_move_counter[move] += cnt\n",
    "    if (iteration + 1) % (MAX_ITERATIONS // 10) == 0:\n",
    "        bucket_move_counters.append(local_move_counter.copy())\n",
    "\n",
    "    # Compute advantages and returns\n",
    "    advantages, returns = compute_advantages(rewards, values, dones)\n",
    "    states = torch.stack(states)\n",
    "    actions = torch.stack(actions)\n",
    "    old_log_probs = torch.stack(old_log_probs).detach()\n",
    "    advantages = torch.stack(advantages).detach()\n",
    "    returns = torch.stack(returns).detach()\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "    # PPO updates\n",
    "    for _ in range(PPO_EPOCHS):\n",
    "        logits, value = model(states)\n",
    "        dist = Categorical(logits=logits)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "        ratio = (new_log_probs - old_log_probs).exp()\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - CLIP_EPS, 1 + CLIP_EPS) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = (returns - value.squeeze()).pow(2).mean()\n",
    "        loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    last_score = episode_scores[-1] if episode_scores else 0.0\n",
    "    score_history.append(last_score)\n",
    "    print(f\"Iter {iteration+1}/{MAX_ITERATIONS}, Last Score: {last_score:.2f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if len(score_history) == ROLLING_AVG_WINDOW:\n",
    "        rolling_avg = sum(score_history) / ROLLING_AVG_WINDOW\n",
    "        if rolling_avg >= ROLLING_AVG_THRESHOLD:\n",
    "            print(f\"Stopping early at iter {iteration+1}: rolling avg = {rolling_avg:.2f}\")\n",
    "            break\n",
    "\n",
    "# --- Final Move Summary ---\n",
    "print(\"\\n=== Final Move Frequency (Total) ===\")\n",
    "for move in ACTION_MAP.values():\n",
    "    print(f\"{move}: {global_move_counter[move]}\")\n",
    "\n",
    "print(\"\\n=== Move Distribution Per 10% of Training ===\")\n",
    "for i, snap in enumerate(bucket_move_counters):\n",
    "    start = i * (MAX_ITERATIONS // 10) + 1\n",
    "    end = (i + 1) * (MAX_ITERATIONS // 10)\n",
    "    print(f\"\\n--- Iterations {start} to {end} ---\")\n",
    "    prev = bucket_move_counters[i-1] if i>0 else None\n",
    "    diff = {move: snap.get(move,0) - (prev.get(move,0) if prev else 0) for move in ACTION_MAP.values()}\n",
    "    for move, cnt in diff.items():\n",
    "        print(f\"{move}: {cnt}\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(f\"\\n🔍 Evaluating over {EVAL_ITERS} episodes…\")\n",
    "eval_scores = []\n",
    "eval_move_counter = defaultdict(int)\n",
    "tile_counts = Counter()\n",
    "\n",
    "for ep in range(EVAL_ITERS):\n",
    "    obs, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        state = preprocess(obs)\n",
    "        logits, _ = model(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "\n",
    "        # get probabilities and sort actions by descending prob\n",
    "        probs = dist.probs.detach().cpu().numpy()\n",
    "        candidates = list(np.argsort(probs)[::-1])\n",
    "\n",
    "        # try each candidate at most once\n",
    "        step_taken = False\n",
    "        for action in candidates:\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            if not info.get('invalid_move', False):\n",
    "                step_taken = True\n",
    "                break\n",
    "            # invalid move → env state unchanged, so safe to try next\n",
    "        if not step_taken:\n",
    "            # fallback to one sampled action\n",
    "            action = int(dist.sample().item())\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # record results\n",
    "        move = ACTION_MAP[action]\n",
    "        eval_move_counter[move] += 1\n",
    "        total_reward += reward\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    eval_scores.append(total_reward)\n",
    "    final_board = decode_board(obs)\n",
    "    max_tile = final_board.max()\n",
    "    tile_counts[max_tile] += 1\n",
    "\n",
    "print(\"\\n✅ Eval Results:\")\n",
    "print(f\"Avg Score: {np.mean(eval_scores):.2f}, Max Score: {np.max(eval_scores):.2f}\")\n",
    "print(\"\\nMax Tile Frequencies:\")\n",
    "for tile, cnt in sorted(tile_counts.items(), reverse=True):\n",
    "    print(f\"{tile}: {cnt} times\")\n",
    "print(\"\\nEval Move Distribution:\")\n",
    "for move in ACTION_MAP.values():\n",
    "    print(f\"{move}: {eval_move_counter[move]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2048",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
