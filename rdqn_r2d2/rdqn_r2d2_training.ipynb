{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import gymnasium as gym\n",
        "import gymnasium_2048\n",
        "from rdqn_r2d2 import RDQNR2D2Agent\n",
        "\n",
        "# yo let's make sure we get consistent results\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's get this party started\n",
        "env = gym.make(\"gymnasium_2048/TwentyFortyEight-v0\")\n",
        "\n",
        "# helper function to decode the board state\n",
        "def decode_board(obs):\n",
        "    if obs.ndim == 3:\n",
        "        idxs = np.argmax(obs, axis=-1)\n",
        "        mask = (obs.sum(axis=-1) == 1)\n",
        "        return (2 ** idxs) * mask\n",
        "    return obs\n",
        "\n",
        "# preprocess function to convert board to our format\n",
        "def preprocess(obs):\n",
        "    board = decode_board(obs).astype(int)\n",
        "    idxs = np.zeros_like(board, dtype=int)\n",
        "    nonzero = board > 0\n",
        "    idxs[nonzero] = np.log2(board[nonzero]).astype(int)\n",
        "    return idxs.flatten()\n",
        "\n",
        "# initialize our awesome agent\n",
        "agent = RDQNR2D2Agent(\n",
        "    state_dim=16,  # 4x4 board\n",
        "    action_dim=4,  # up, down, left, right - the classics\n",
        "    gamma=0.99,    # gotta think about the future\n",
        "    n_step=5,      # look ahead a bit\n",
        "    sequence_length=20,  # remember the past\n",
        "    burn_in_length=5,   # warm up that lstm\n",
        "    learning_rate=0.0001,  # slow and steady wins the race\n",
        "    target_update_freq=1000,  # keep that target net fresh\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"  # use that gpu if ya got it\n",
        ")\n",
        "\n",
        "# training settings - feel free to tweak these\n",
        "num_episodes = 10000\n",
        "batch_size = 32\n",
        "epsilon_start = 1.0   # start exploring a lot\n",
        "epsilon_end = 0.01    # end with mostly exploitation\n",
        "epsilon_decay = 0.995  # smooth decay\n",
        "max_steps_per_episode = 2000  # don't let episodes run forever\n",
        "\n",
        "# keep track of how we're doing\n",
        "episode_rewards = []\n",
        "max_tiles = []\n",
        "episode_lengths = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# alright, let's train this bad boy\n",
        "epsilon = epsilon_start\n",
        "hidden = None  # start with fresh lstm state\n",
        "\n",
        "for episode in tqdm(range(num_episodes)):\n",
        "    state, _ = env.reset()  # gymnasium style\n",
        "    state = preprocess(state)  # convert to our format\n",
        "    episode_reward = 0\n",
        "    agent.current_sequence = []  # fresh sequence for each episode\n",
        "    \n",
        "    for step in range(max_steps_per_episode):\n",
        "        # pick an action\n",
        "        action, new_hidden = agent.select_action(state, hidden, epsilon)\n",
        "        hidden = new_hidden\n",
        "        \n",
        "        # take that action and see what happens\n",
        "        next_state, reward, done, truncated, info = env.step(action)  # gymnasium style\n",
        "        next_state = preprocess(next_state)\n",
        "        done = done or truncated  # combine terminal conditions\n",
        "        \n",
        "        # add to our sequence memory\n",
        "        agent.current_sequence.append((state, action, reward, next_state, done))\n",
        "        \n",
        "        # if sequence is full or episode ends, save it\n",
        "        if len(agent.current_sequence) == agent.sequence_length or done:\n",
        "            agent.replay_buffer.push(agent.current_sequence)\n",
        "            agent.current_sequence = []\n",
        "            hidden = None  # reset lstm state for new sequence\n",
        "        \n",
        "        # keep track of rewards\n",
        "        episode_reward += reward\n",
        "        state = next_state\n",
        "        \n",
        "        # learn from experience if we have enough\n",
        "        if len(agent.replay_buffer.buffer) > batch_size:\n",
        "            loss = agent.update(batch_size)\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    # track our progress\n",
        "    episode_rewards.append(episode_reward)\n",
        "    max_tiles.append(np.max(decode_board(env.get_board())))  # get max tile from board\n",
        "    episode_lengths.append(step + 1)\n",
        "    \n",
        "    # reduce exploration over time\n",
        "    epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "    \n",
        "    # brag about our progress every 100 episodes\n",
        "    if (episode + 1) % 100 == 0:\n",
        "        avg_reward = np.mean(episode_rewards[-100:])\n",
        "        avg_max_tile = np.mean(max_tiles[-100:])\n",
        "        avg_length = np.mean(episode_lengths[-100:])\n",
        "        print(f\"\\nEpisode {episode + 1} - how're we doing?\")\n",
        "        print(f\"Average Reward (last 100): {avg_reward:.2f}\")\n",
        "        print(f\"Average Max Tile (last 100): {avg_max_tile:.2f}\")\n",
        "        print(f\"Average Episode Length (last 100): {avg_length:.2f}\")\n",
        "        print(f\"Current Epsilon: {epsilon:.3f} (still exploring!)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's see how we did with some pretty plots\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# show those sweet rewards\n",
        "plt.subplot(131)\n",
        "plt.plot(episode_rewards, alpha=0.6)\n",
        "plt.plot(np.convolve(episode_rewards, np.ones(100)/100, mode='valid'), \n",
        "         label='100-ep avg', color='red')\n",
        "plt.title('rewards over time')\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('total reward')\n",
        "plt.legend()\n",
        "\n",
        "# check out our max tiles\n",
        "plt.subplot(132)\n",
        "plt.plot(max_tiles, alpha=0.6)\n",
        "plt.plot(np.convolve(max_tiles, np.ones(100)/100, mode='valid'),\n",
        "         label='100-ep avg', color='red')\n",
        "plt.title('highest tile reached')\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('max tile value')\n",
        "plt.legend()\n",
        "\n",
        "# how long did episodes last?\n",
        "plt.subplot(133)\n",
        "plt.plot(episode_lengths, alpha=0.6)\n",
        "plt.plot(np.convolve(episode_lengths, np.ones(100)/100, mode='valid'),\n",
        "         label='100-ep avg', color='red')\n",
        "plt.title('episode length')\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('steps')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# save our hard work for later\n",
        "print(\"\\nsaving the model... \", end=\"\")\n",
        "torch.save({\n",
        "    'online_net_state_dict': agent.online_net.state_dict(),\n",
        "    'target_net_state_dict': agent.target_net.state_dict(),\n",
        "    'optimizer_state_dict': agent.optimizer.state_dict(),\n",
        "    'training_history': {\n",
        "        'rewards': episode_rewards,\n",
        "        'max_tiles': max_tiles,\n",
        "        'lengths': episode_lengths\n",
        "    }\n",
        "}, 'rdqn_r2d2_model.pth')\n",
        "print(\"done! ðŸŽ‰\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's see how well our agent plays\n",
        "def evaluate_agent(agent, env, num_episodes=10, render=False):\n",
        "    total_rewards = []\n",
        "    max_tiles = []\n",
        "    episode_lengths = []\n",
        "    hidden = None\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()  # gymnasium style\n",
        "        state = preprocess(state)  # convert to our format\n",
        "        episode_reward = 0\n",
        "        steps = 0\n",
        "        \n",
        "        while True:\n",
        "            if render:\n",
        "                env.render()\n",
        "            \n",
        "            # let the agent do its thing (no random actions)\n",
        "            action, new_hidden = agent.select_action(state, hidden, epsilon=0.0)\n",
        "            hidden = new_hidden\n",
        "            \n",
        "            # take the action\n",
        "            next_state, reward, done, truncated, info = env.step(action)  # gymnasium style\n",
        "            next_state = preprocess(next_state)\n",
        "            done = done or truncated\n",
        "            \n",
        "            episode_reward += reward\n",
        "            steps += 1\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "            state = next_state\n",
        "        \n",
        "        total_rewards.append(episode_reward)\n",
        "        max_tiles.append(np.max(decode_board(env.get_board())))\n",
        "        episode_lengths.append(steps)\n",
        "    \n",
        "    return {\n",
        "        'avg_reward': np.mean(total_rewards),\n",
        "        'avg_max_tile': np.mean(max_tiles),\n",
        "        'avg_length': np.mean(episode_lengths),\n",
        "        'max_tile_achieved': max(max_tiles)\n",
        "    }\n",
        "\n",
        "# time for the moment of truth!\n",
        "print(\"\\nrunning some test games...\")\n",
        "eval_results = evaluate_agent(agent, env, num_episodes=10, render=False)\n",
        "print(\"\\nhow'd we do? ðŸ¤”\")\n",
        "print(f\"Average Reward: {eval_results['avg_reward']:.2f}\")\n",
        "print(f\"Average Max Tile: {eval_results['avg_max_tile']:.2f}\")\n",
        "print(f\"Average Episode Length: {eval_results['avg_length']:.2f}\")\n",
        "print(f\"Highest Tile Reached: {eval_results['max_tile_achieved']} ðŸŽ®\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
